import express from "express";
import fetch from "node-fetch";

const app = express();
app.use(express.json());

const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY;

app.get("/", (_, res) => res.send("âœ… Gemini Proxy + OpenRouter Switch Ready"));

// ---------------------------
// å…±é€šè£œåŠ©é–¢æ•°
// ---------------------------
async function safeJson(res) {
  const text = await res.text();
  try {
    return JSON.parse(text);
  } catch {
    console.error("âš ï¸ Invalid JSON:", text.slice(0, 500));
    return {};
  }
}

// ---------------------------
// OpenAIäº’æ›API
// ---------------------------
app.post("/v1/chat/completions", async (req, res) => {
  const { model = "gemini-pro", messages } = req.body;

  try {
    let answer = "";

    // ====== OpenRouterãƒ¢ãƒ‡ãƒ«æŒ‡å®šæ™‚ ======
    if (model.startsWith("openrouter:")) {
      const routerModel = model.replace("openrouter:", "");
      console.log(`ğŸ” Routing to OpenRouter model: ${routerModel}`);

      const routerRes = await fetch("https://openrouter.ai/api/v1/chat/completions", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${OPENROUTER_API_KEY}`,
          "HTTP-Referer": "https://your-app.onrender.com",
          "X-Title": "Gemini Proxy",
        },
        body: JSON.stringify({
          model: routerModel,
          messages,
        }),
      });

      const routerData = await safeJson(routerRes);
      answer =
        routerData?.choices?.[0]?.message?.content ||
        "ï¼ˆOpenRouterã‹ã‚‰å›ç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰";
    }

    // ====== Geminiãƒ¢ãƒ‡ãƒ«æŒ‡å®šæ™‚ ======
    else if (model.startsWith("gemini")) {
      console.log(`ğŸ” Routing to Gemini model: ${model}`);

      // Geminiç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
      const userText = messages
        .map(m => `${m.role}: ${m.content}`)
        .join("\n");

      const geminiRes = await fetch(
        `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${GEMINI_API_KEY}`,
        {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            contents: [{ parts: [{ text: userText }] }],
          }),
        }
      );

      const geminiData = await safeJson(geminiRes);
      answer =
        geminiData?.candidates?.[0]?.content?.parts?.[0]?.text ||
        "ï¼ˆGeminiã‹ã‚‰å›ç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰";
    }

    // ====== ä¸æ˜ãƒ¢ãƒ‡ãƒ«æ™‚ ======
    else {
      answer = `ï¼ˆå¯¾å¿œã—ã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«æŒ‡å®šã§ã™: ${model}ï¼‰`;
    }

    // ---------------------------
    // OpenAIäº’æ›å½¢å¼ã§è¿”ã™
    // ---------------------------
    res.json({
      id: `chatcmpl-${Date.now()}`,
      object: "chat.completion",
      created: Math.floor(Date.now() / 1000),
      model,
      choices: [
        {
          index: 0,
          message: { role: "assistant", content: answer },
          finish_reason: "stop",
        },
      ],
    });
  } catch (err) {
    console.error(err);
    res.status(500).json({ error: err.message });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () =>
  console.log(`ğŸŒ Proxy server running on port ${PORT}`)
);
