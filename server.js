import express from "express";
import fetch from "node-fetch";

const app = express();
app.use(express.json());

const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY;
const TAVILY_API_KEY = process.env.TAVILY_API_KEY;
const DEFAULT_GEMINI_MODEL = "models/gemini-2.5-flash-lite";

// ã‚µãƒ¼ãƒ“ã‚¹ç¨¼åƒç¢ºèª
app.get("/", (_, res) =>
  res.send(`âœ… Gemini + OpenRouter + Tavily Proxy running`)
);

// OpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
app.post("/v1/chat/completions", async (req, res) => {
  try {
    const { model, messages } = req.body;
    if (!model) return res.status(400).json({ error: "Model is required" });
    if (!messages || !Array.isArray(messages) || messages.length === 0)
      return res.status(400).json({ error: "No messages found" });

    // -----------------------------
    // ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¦ç´„
    // -----------------------------
    const systemPrompt = messages.find(m => m.role === "system")?.content || "";
    // ã“ã“ã¯å¿…è¦ã«å¿œã˜ã¦è¦ç´„é–¢æ•°ã‚’å‘¼ã¹ã‚‹
    // const summarizedSystemPrompt = await summarizeSystemPrompt(systemPrompt);
    const summarizedSystemPrompt = systemPrompt;

    // -----------------------------
    // æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã ã‘ã‚’æ¤œç´¢ã«ä½¿ç”¨
    // -----------------------------
    const lastUserMessage = messages.filter(m => m.role === "user").slice(-1)[0]?.content;
    if (!lastUserMessage) return res.status(400).json({ error: "No user text found" });

    // -----------------------------
    // Tavilyæ¤œç´¢ï¼ˆä»»æ„ï¼‰
    // -----------------------------
    let context = "ï¼ˆæ¤œç´¢çµæœãªã—ï¼‰";
    if (TAVILY_API_KEY) {
      try {
        const tavilyRes = await fetch("https://api.tavily.com/search", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${TAVILY_API_KEY}`,
          },
          body: JSON.stringify({ query: lastUserMessage, max_results: 3 }),
        });
        const tavilyData = await safeJson(tavilyRes);
        context =
          tavilyData.results?.map(r => `- ${r.title}\n${r.content}`).join("\n\n") || context;
      } catch (err) {
        console.error("Tavily search error:", err);
      }
    }

    // -----------------------------
    // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    // -----------------------------
    const chatHistory = messages
      .filter(m => m.role !== "system")
      .map(m => `${m.role}: ${m.content}`).join("\n\n");

    const promptWithContext = `
ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:
${summarizedSystemPrompt}

æ¤œç´¢çµæœ:
${context}

ä¼šè©±å±¥æ­´:
${chatHistory}
`;

    // -----------------------------
    // ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å‘¼ã³å‡ºã—
    // -----------------------------
    let answer = "ï¼ˆå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰";
    if (model.startsWith("gemini:")) {
      const geminiModel = model.replace("gemini:", "") || DEFAULT_GEMINI_MODEL;
      answer = await callGemini(geminiModel, promptWithContext);
    } else if (model.startsWith("openrouter:")) {
      const orModel = model.replace("openrouter:", "");
      answer = await callOpenRouter(orModel, messages);
    } else {
      return res.status(400).json({ error: "Unknown model prefix" });
    }

    // -----------------------------
    // OpenAIäº’æ›ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    // -----------------------------
    res.json({
      id: `chatcmpl-${Date.now()}`,
      object: "chat.completion",
      created: Math.floor(Date.now() / 1000),
      model,
      choices: [
        { index: 0, message: { role: "assistant", content: answer }, finish_reason: "stop" },
      ],
    });
  } catch (err) {
    console.error(err);
    res.status(500).json({ error: err.message });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`ğŸŒ Server running on port ${PORT}`));

// -----------------------------
// Geminiå‘¼ã³å‡ºã—
// -----------------------------
async function callGemini(model, prompt) {
  try {
    const res = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/${model}:generateContent?key=${GEMINI_API_KEY}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          contents: [{ role: "user", parts: [{ text: prompt }] }],
          safetySettings: [
            { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_NONE" },
            { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_NONE" },
            { category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_NONE" },
            { category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_NONE" }
          ]
        }),
      }
    );
    const data = await safeJson(res);
    return data?.candidates?.[0]?.content?.parts?.[0]?.text || "ï¼ˆGeminiã‹ã‚‰å›ç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰";
  } catch (err) {
    console.error("Gemini call error:", err);
    return "ï¼ˆGeminiå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰";
  }
}

// -----------------------------
// OpenRouterå‘¼ã³å‡ºã—
// -----------------------------
async function callOpenRouter(model, messages) {
  try {
    const res = await fetch("https://openrouter.ai/api/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${OPENROUTER_API_KEY}`,
      },
      body: JSON.stringify({
        model,
        messages,        // OpenAIäº’æ›å½¢å¼ã§é€ä¿¡
        stream: false,   // å¿µã®ãŸã‚æ˜ç¤º
        max_tokens: 1024 // å¿…è¦ã«å¿œã˜ã¦èª¿æ•´
      }),
    });
    const data = await safeJson(res);
    return data?.choices?.[0]?.message?.content || "ï¼ˆOpenRouterã‹ã‚‰å›ç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰";
  } catch (err) {
    console.error("OpenRouter call error:", err);
    return "ï¼ˆOpenRouterå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰";
  }
}


// -----------------------------
// å®‰å…¨JSONè§£æ
// -----------------------------
async function safeJson(res) {
  const text = await res.text();
  try { return JSON.parse(text); } 
  catch { 
    console.error("âš ï¸ Invalid JSON:", text.slice(0, 500));
    return {};
  }
}

// -----------------------------
// ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¦ç´„ï¼ˆå¿…è¦ã«å¿œã˜ã¦ä½¿ç”¨ï¼‰
// -----------------------------
async function summarizeSystemPrompt(systemPrompt) {
  if (!systemPrompt) return "";
  try {
    const summaryRes = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/${DEFAULT_GEMINI_MODEL}:generateContent?key=${GEMINI_API_KEY}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          contents: [
            {
              role: "user",
              parts: [
                {
                  text: `ä»¥ä¸‹ã®æ–‡ç« ã‚’ã€é•·ã•ã‚’7å‰²ç¨‹åº¦ã«æŠ‘ãˆã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªæƒ…å ±ã¯æ®‹ã—ã¦ãã ã•ã„:\n\n${systemPrompt}`
                }
              ]
            }
          ]
        })
      }
    );
    const data = await safeJson(summaryRes);
    return data?.candidates?.[0]?.content?.parts?.[0]?.text || systemPrompt;
  } catch (err) {
    console.error("System prompt summarization error:", err);
    return systemPrompt;
  }
}
