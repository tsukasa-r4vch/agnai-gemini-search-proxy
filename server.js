import express from "express";
import fetch from "node-fetch";

const app = express();
app.use(express.json());

const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const GEMINI_MODEL = process.env.GEMINI_MODEL || "models/gemini-2.5-flash-lite";
const TAVILY_API_KEY = process.env.TAVILY_API_KEY;

app.get("/", (_, res) =>
  res.send(`âœ… Gemini + Tavily Proxy running on model: ${GEMINI_MODEL}`)
);

app.post("/v1/chat/completions", async (req, res) => {
  try {
    const { model, messages } = req.body;
    const selectedModel = model || GEMINI_MODEL;

    if (!messages || !Array.isArray(messages) || messages.length === 0) {
      return res.status(400).json({ error: "No messages found" });
    }

    // ğŸ”¹ messages ã‚’çµåˆã—ã¦ãƒ¦ãƒ¼ã‚¶è³ªå•ã‚’æŠ½å‡º
    const userMessages = messages
      .filter((m) => m.role === "user")
      .map((m) => m.content)
      .join("\n");

    if (!userMessages) {
      return res.status(400).json({ error: "No user text found" });
    }

    // ğŸ” Tavily æ¤œç´¢
    let context = "ï¼ˆæ¤œç´¢çµæœãªã—ï¼‰";
    if (TAVILY_API_KEY) {
      try {
        const tavilyRes = await fetch("https://api.tavily.com/search", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${TAVILY_API_KEY}`,
          },
          body: JSON.stringify({ query: userMessages, max_results: 3 }),
        });
        const tavilyData = await safeJson(tavilyRes);
        context =
          tavilyData.results?.map((r) => `- ${r.title}\n${r.content}`).join("\n\n") ||
          context;
      } catch (err) {
        console.error("Tavily search error:", err);
      }
    }

    // ğŸ”¹ Gemini ã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    const fullPrompt = messages
      .map((m) => {
        const roleLabel =
          m.role === "system"
            ? "ã‚·ã‚¹ãƒ†ãƒ "
            : m.role === "user"
            ? "ãƒ¦ãƒ¼ã‚¶ãƒ¼"
            : "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ";
        return `${roleLabel}: ${m.content}`;
      })
      .join("\n\n");

    const promptWithContext = `
æ¬¡ã®æ¤œç´¢çµæœã‚’ã‚‚ã¨ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚

æ¤œç´¢çµæœ:
${context}

${fullPrompt}
`;

    // ğŸ”¹ Gemini API å‘¼ã³å‡ºã—
    const geminiRes = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/${selectedModel}:generateContent?key=${GEMINI_API_KEY}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          contents: [{ role: "user", parts: [{ text: promptWithContext }] }],
        }),
      }
    );

    const geminiData = await safeJson(geminiRes);

    const answer =
      geminiData?.candidates?.[0]?.content?.parts?.[0]?.text ||
      "ï¼ˆGeminiã‹ã‚‰å›ç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰";

    // ğŸ”¹ OpenAIäº’æ›ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    res.json({
      id: `chatcmpl-${Date.now()}`,
      object: "chat.completion",
      created: Math.floor(Date.now() / 1000),
      model: selectedModel,
      choices: [
        {
          index: 0,
          message: { role: "assistant", content: answer },
          finish_reason: "stop",
        },
      ],
    });
  } catch (err) {
    console.error(err);
    res.status(500).json({ error: err.message });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () =>
  console.log(`ğŸŒ Server running on port ${PORT} (model: ${GEMINI_MODEL})`)
);

// ğŸ”¹ å®‰å…¨ãª JSON è§£æ
async function safeJson(res) {
  const text = await res.text();
  try {
    return JSON.parse(text);
  } catch {
    console.error("âš ï¸ Invalid JSON:", text.slice(0, 500));
    return {};
  }
}
